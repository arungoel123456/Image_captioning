{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/dataset/TextFiles/Flickr8k.token.txt\"\n",
    "file = open(filename, 'r')\n",
    "doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    \n",
    "    # take the first token as image id, the rest as description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    \n",
    "    # extract filename from image id\n",
    "    image_id = image_id.split('.')[0]\n",
    "    \n",
    "    # convert description tokens back to string\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in mapping:\n",
    "        descriptions[image_id] = list()\n",
    "    descriptions[image_id].append(image_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        desc = desc_list[i]\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove tokens with numbers in them\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # store as string\n",
    "        desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a set of all unique words\n",
    "vocabulary = set()\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions[key]]\n",
    "print('Original Vocabulary Size: %d' % len(vocabulary))\n",
    "Original Vocabulary Size: 8763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now considering only those words that are present more than 10times\n",
    "\n",
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "# Consider only words which occur at least 10 times in the corpus\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "print('preprocessed words %d ' % len(vocab)))\n",
    "# preprocessed words 1651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training dataset\n",
    "filename = 'dataset/TextFiles/Flickr_8k.trainImages.txt'\n",
    "doc = load_doc(filename)\n",
    "train = list()\n",
    "for line in doc.split('\\n'):\n",
    "    identifier = line.split('.')[0]\n",
    "    train.append(identifier)\n",
    "print('Dataset: %d' % len(train))\n",
    "Dataset: 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('descriptions.txt')\n",
    "train_descriptions = dict()\n",
    "for line in doc.split('\\n'):\n",
    "    # split line by white space\n",
    "    tokens = line.split()\n",
    "    \n",
    "    # split id from description\n",
    "    image_id, image_desc = tokens[0], tokens[1:]\n",
    "    \n",
    "    # skip images not in the set\n",
    "    if image_id in dataset:\n",
    "        if image_id not in descriptions:\n",
    "            train_descriptions[image_id] = list()\n",
    "        \n",
    "        # wrap description in tokens\n",
    "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "        \n",
    "        # store\n",
    "        train_descriptions[image_id].append(desc)\n",
    "\n",
    "        print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# Descriptions: train=6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the InceptionV3 model trained on imagenet data\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Remove the last layer (output softmax layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now passing every image to this model\n",
    "\n",
    "# Convert all the images to size 299x299 as expected by the\n",
    "# inception v3 model\n",
    "img = image.load_img(image_path, target_size=(299, 299))\n",
    "# Convert PIL image to numpy array of 3-dimensions\n",
    "x = image.img_to_array(img)\n",
    "# Add one more dimension\n",
    "x = np.expand_dims(x, axis=0)\n",
    "# preprocess images using preprocess_input() from inception module\n",
    "x = preprocess_input(x)\n",
    "# reshape from (1, 2048) to (2048, )\n",
    "x = np.reshape(x, x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting word to index and vice versa\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating max-description length\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    " all_desc = list()\n",
    " for key in descriptions.keys():\n",
    "  [all_desc.append(d) for d in descriptions[key]]\n",
    " return all_desc\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    " lines = to_lines(descriptions)\n",
    " return max(len(d.split()) for d in lines)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Max Description Length: %d' % max_length)\n",
    "Max Description Length: 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation using Generator Function\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings\n",
    "\n",
    "# Load Glove vectors\n",
    "glove_dir = 'dataset/glove'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss=’categorical_crossentropy’, optimizer=’adam’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySearch(photo):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
